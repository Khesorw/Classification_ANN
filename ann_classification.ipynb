{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121cc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaf3076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(3,4)\n",
    "\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5d77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1797, 64)\n",
      "y shape: (1797,)\n",
      "Train: (1257, 64) | Test: (540, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data        # shape (1797, 64)\n",
    "y = digits.target      # labels 0â€“9\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d4540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 472  217  391 1388 1382  529  787 1275  638 1611]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "idx = np.random.choice(len(X), 10, replace=False)\n",
    "print(idx)\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "for i, img_idx in enumerate(idx):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(X[img_idx].reshape(-1,8), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {y[img_idx]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b93c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m y_train_t = torch.tensor(y_train,dtype=torch.long)\n\u001b[32m     12\u001b[39m y_test_t = torch.tensor(y_test,dtype=torch.long)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28mprint\u001b[39m(y_test_t.dtype,x_test_t.dtype)\n",
      "\u001b[31mNameError\u001b[39m: name 'x_test_t' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_norm = scaler.fit_transform(X_train)\n",
    "x_test_norm = scaler.fit_transform(X_test)\n",
    "\n",
    "#converting them to tensor\n",
    "\n",
    "X_train_t = torch.tensor(x_train_norm,dtype=torch.float32)\n",
    "X_test_t = torch.tensor(x_test_norm,dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train,dtype=torch.long)\n",
    "y_test_t = torch.tensor(y_test,dtype=torch.long)\n",
    "\n",
    "\n",
    "print(y_test_t.dtype,x_test_t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "967f7b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def onehot(label):\n",
    "    return torch.eye(10)[label]\n",
    "\n",
    "y_train_oh = onehot(y_train_t)\n",
    "y_test_oh  = onehot(y_test_t)\n",
    "print(y_test_oh[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84205805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 0.0000\n",
      "Epoch 5, Loss = 0.0000\n",
      "Epoch 10, Loss = 0.0000\n",
      "Epoch 15, Loss = 0.0000\n",
      "Epoch 20, Loss = 0.0000\n",
      "Epoch 25, Loss = 0.0000\n",
      "Epoch 30, Loss = 0.0000\n",
      "Epoch 35, Loss = 0.0000\n",
      "Epoch 40, Loss = 0.0000\n",
      "Epoch 45, Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    " \n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i : i + batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_oh[idx]\n",
    "\n",
    "       \n",
    "        y_pred = model.forward(X_batch)\n",
    "\n",
    " \n",
    "        model.back_prop(y_batch, lr=lr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {model.Loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b82a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, num_neurons, num_classes=10):\n",
    "        #nn.Parameter when initializing\n",
    "        self.w1 = torch.randn(input_dim, num_neurons) * 0.01\n",
    "        self.w1 = torch.nn.Parameter(self.w1)\n",
    "\n",
    "        self.b1 = torch.zeros(1, num_neurons)\n",
    "        self.b1 = torch.nn.Parameter(self.b1)\n",
    "\n",
    "        self.w2 = torch.randn(num_neurons, num_classes) * 0.01\n",
    "        self.w2 = torch.nn.Parameter(self.w2)\n",
    "\n",
    "        self.b2 = torch.zeros(1, num_classes)\n",
    "        self.b2 = torch.nn.Parameter(self.b2)\n",
    "\n",
    "        self.Loss = []\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return torch.relu(x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        ex = torch.exp(x - x.max(dim=1, keepdim=True).values)\n",
    "        return ex / ex.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #loss cross-entropy \n",
    "    def cross_entropy(self, predicted, target):\n",
    "        per_sample = -torch.sum(target * torch.log(predicted + 1e-12), dim=1)\n",
    "        return per_sample.mean()\n",
    "\n",
    "    # ---- Forward ----\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.w1 + self.b1\n",
    "        self.h1 = self.ReLU(self.z1)\n",
    "        self.z2 = self.h1 @ self.w2 + self.b2\n",
    "        self.y_hat = self.softmax(self.z2)\n",
    "        return self.y_hat\n",
    "\n",
    "    # ---- Backward ----\n",
    "    def back_prop(self, target, lr=1e-3):\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.cross_entropy(self.y_hat, target)\n",
    "        self.Loss.append(loss.item())\n",
    "\n",
    "        # clear old gradients\n",
    "        for p in [self.w1, self.b1, self.w2, self.b2]:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "     \n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.w1 -= lr * self.w1.grad\n",
    "            self.b1 -= lr * self.b1.grad\n",
    "            self.w2 -= lr * self.w2.grad\n",
    "            self.b2 -= lr * self.b2.grad\n",
    "\n",
    "        # re-enable grad\n",
    "        self.w1.requires_grad_(True)\n",
    "        self.b1.requires_grad_(True)\n",
    "        self.w2.requires_grad_(True)\n",
    "        self.b2.requires_grad_(True)\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdb4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=23\n",
    ")\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "# convert to tensors\n",
    "X_train_t = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_norm, dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# one-hot\n",
    "y_train_oh = torch.eye(10)[y_train_t]\n",
    "y_test_oh  = torch.eye(10)[y_test_t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70234fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100]) \n",
      "\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(input_dim=64, num_neurons=100, num_classes=10)\n",
    "print(model.w1.shape,'\\n')\n",
    "print(model.w2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a082ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 2.3030\n",
      "Epoch 5, Loss = 2.3022\n",
      "Epoch 10, Loss = 2.2731\n",
      "Epoch 15, Loss = 2.2800\n",
      "Epoch 20, Loss = 2.2195\n",
      "Epoch 25, Loss = 2.1814\n",
      "Epoch 30, Loss = 2.0918\n",
      "Epoch 35, Loss = 1.9220\n",
      "Epoch 40, Loss = 1.6179\n",
      "Epoch 45, Loss = 1.6619\n",
      "Epoch 50, Loss = 1.1582\n",
      "Epoch 55, Loss = 1.3060\n",
      "Epoch 60, Loss = 0.8785\n",
      "Epoch 65, Loss = 0.7021\n",
      "Epoch 70, Loss = 0.6570\n",
      "Epoch 75, Loss = 0.5273\n",
      "Epoch 80, Loss = 0.5852\n",
      "Epoch 85, Loss = 0.8344\n",
      "Epoch 90, Loss = 0.7739\n",
      "Epoch 95, Loss = 0.2197\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lr = 0.01          \n",
    "epochs = 100\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_oh[idx]\n",
    "\n",
    "        model.forward(X_batch)\n",
    "        loss = model.back_prop(y_batch, lr=lr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1edfd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9203703999519348\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.forward(X_test_t)\n",
    "pred_labels = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "acc = (pred_labels == y_test_t).float().mean().item()\n",
    "print(\"Test Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3eb5a",
   "metadata": {},
   "source": [
    "Now classification with nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19f6aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "seq = nn.Sequential(\n",
    "    nn.Linear(64,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,10)\n",
    "    \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7316247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = nn.Sequential(\n",
    "    nn.Linear(64,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,10)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(seq.parameters(),lr=1e-3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924048e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.0341 | Test Acc: 0.9667\n",
      "Epoch 05 | Loss: 0.0398 | Test Acc: 0.9667\n",
      "Epoch 10 | Loss: 0.1370 | Test Acc: 0.9667\n",
      "Epoch 15 | Loss: 0.0610 | Test Acc: 0.9667\n",
      "Epoch 20 | Loss: 0.0357 | Test Acc: 0.9667\n",
      "Epoch 25 | Loss: 0.0885 | Test Acc: 0.9667\n",
      "Epoch 30 | Loss: 0.1095 | Test Acc: 0.9685\n",
      "Epoch 35 | Loss: 0.0629 | Test Acc: 0.9667\n",
      "Epoch 40 | Loss: 0.0578 | Test Acc: 0.9667\n",
      "Epoch 45 | Loss: 0.0629 | Test Acc: 0.9667\n",
      "Epoch 50 | Loss: 0.0509 | Test Acc: 0.9667\n",
      "Epoch 55 | Loss: 0.0602 | Test Acc: 0.9667\n",
      "Epoch 60 | Loss: 0.0511 | Test Acc: 0.9667\n",
      "Epoch 65 | Loss: 0.0689 | Test Acc: 0.9667\n",
      "Epoch 70 | Loss: 0.0470 | Test Acc: 0.9667\n",
      "Epoch 75 | Loss: 0.0333 | Test Acc: 0.9667\n",
      "Epoch 80 | Loss: 0.0245 | Test Acc: 0.9667\n",
      "Epoch 85 | Loss: 0.0880 | Test Acc: 0.9667\n",
      "Epoch 90 | Loss: 0.2546 | Test Acc: 0.9667\n",
      "Epoch 95 | Loss: 0.0453 | Test Acc: 0.9667\n",
      "Epoch 100 | Loss: 0.0374 | Test Acc: 0.9667\n",
      "Epoch 105 | Loss: 0.0756 | Test Acc: 0.9667\n",
      "Epoch 110 | Loss: 0.0827 | Test Acc: 0.9667\n",
      "Epoch 115 | Loss: 0.0412 | Test Acc: 0.9667\n",
      "Epoch 120 | Loss: 0.0244 | Test Acc: 0.9667\n",
      "Epoch 125 | Loss: 0.0808 | Test Acc: 0.9667\n",
      "Epoch 130 | Loss: 0.1344 | Test Acc: 0.9667\n",
      "Epoch 135 | Loss: 0.0351 | Test Acc: 0.9667\n",
      "Epoch 140 | Loss: 0.0794 | Test Acc: 0.9667\n",
      "Epoch 145 | Loss: 0.0394 | Test Acc: 0.9667\n",
      "Epoch 150 | Loss: 0.0194 | Test Acc: 0.9667\n",
      "Epoch 155 | Loss: 0.1079 | Test Acc: 0.9667\n",
      "Epoch 160 | Loss: 0.0795 | Test Acc: 0.9667\n",
      "Epoch 165 | Loss: 0.1435 | Test Acc: 0.9667\n",
      "Epoch 170 | Loss: 0.2000 | Test Acc: 0.9667\n",
      "Epoch 175 | Loss: 0.0461 | Test Acc: 0.9667\n",
      "Epoch 180 | Loss: 0.0554 | Test Acc: 0.9667\n",
      "Epoch 185 | Loss: 0.0253 | Test Acc: 0.9667\n",
      "Epoch 190 | Loss: 0.0822 | Test Acc: 0.9667\n",
      "Epoch 195 | Loss: 0.0165 | Test Acc: 0.9667\n",
      "Epoch 200 | Loss: 0.0329 | Test Acc: 0.9667\n",
      "Epoch 205 | Loss: 0.0691 | Test Acc: 0.9667\n",
      "Epoch 210 | Loss: 0.2651 | Test Acc: 0.9667\n",
      "Epoch 215 | Loss: 0.1010 | Test Acc: 0.9667\n",
      "Epoch 220 | Loss: 0.0111 | Test Acc: 0.9667\n",
      "Epoch 225 | Loss: 0.0889 | Test Acc: 0.9667\n",
      "Epoch 230 | Loss: 0.0381 | Test Acc: 0.9667\n",
      "Epoch 235 | Loss: 0.0983 | Test Acc: 0.9667\n",
      "Epoch 240 | Loss: 0.1711 | Test Acc: 0.9667\n",
      "Epoch 245 | Loss: 0.0249 | Test Acc: 0.9667\n",
      "Epoch 250 | Loss: 0.0546 | Test Acc: 0.9667\n",
      "Epoch 255 | Loss: 0.1342 | Test Acc: 0.9667\n",
      "Epoch 260 | Loss: 0.0345 | Test Acc: 0.9667\n",
      "Epoch 265 | Loss: 0.0852 | Test Acc: 0.9667\n",
      "Epoch 270 | Loss: 0.0429 | Test Acc: 0.9667\n",
      "Epoch 275 | Loss: 0.0313 | Test Acc: 0.9667\n",
      "Epoch 280 | Loss: 0.0262 | Test Acc: 0.9667\n",
      "Epoch 285 | Loss: 0.1442 | Test Acc: 0.9667\n",
      "Epoch 290 | Loss: 0.1433 | Test Acc: 0.9667\n",
      "Epoch 295 | Loss: 0.0938 | Test Acc: 0.9667\n",
      "Epoch 300 | Loss: 0.0408 | Test Acc: 0.9667\n",
      "Epoch 305 | Loss: 0.0420 | Test Acc: 0.9667\n",
      "Epoch 310 | Loss: 0.0552 | Test Acc: 0.9667\n",
      "Epoch 315 | Loss: 0.0390 | Test Acc: 0.9667\n",
      "Epoch 320 | Loss: 0.0508 | Test Acc: 0.9667\n",
      "Epoch 325 | Loss: 0.0417 | Test Acc: 0.9667\n",
      "Epoch 330 | Loss: 0.0201 | Test Acc: 0.9667\n",
      "Epoch 335 | Loss: 0.0412 | Test Acc: 0.9667\n",
      "Epoch 340 | Loss: 0.0457 | Test Acc: 0.9667\n",
      "Epoch 345 | Loss: 0.0190 | Test Acc: 0.9667\n",
      "Epoch 350 | Loss: 0.0392 | Test Acc: 0.9667\n",
      "Epoch 355 | Loss: 0.0045 | Test Acc: 0.9667\n",
      "Epoch 360 | Loss: 0.0712 | Test Acc: 0.9667\n",
      "Epoch 365 | Loss: 0.0612 | Test Acc: 0.9667\n",
      "Epoch 370 | Loss: 0.0247 | Test Acc: 0.9667\n",
      "Epoch 375 | Loss: 0.0580 | Test Acc: 0.9667\n",
      "Epoch 380 | Loss: 0.2456 | Test Acc: 0.9667\n",
      "Epoch 385 | Loss: 0.0189 | Test Acc: 0.9667\n",
      "Epoch 390 | Loss: 0.1772 | Test Acc: 0.9667\n",
      "Epoch 395 | Loss: 0.0431 | Test Acc: 0.9667\n",
      "\n",
      "Final Test Accuracy: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "def test_step():\n",
    "    seq.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = seq(X_test_t)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        accuracy = (predictions == y_test_t).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "batch_size = 20\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_t[idx]\n",
    "        seq.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = seq(X_batch)\n",
    "        loss = criterion(logits,y_batch) #ce losss\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lo = loss.item()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        acc = test_step()\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {lo:.4f} | Test Acc: {acc:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Final accuracy\n",
    "# ---------------------------------------------------\n",
    "final_acc = test_step()\n",
    "print(\"\\nFinal Test Accuracy:\", final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5219f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6, 5, 8, 5, 4, 0, 5, 4, 5, 6, 4, 8, 7, 8, 3, 1, 8, 8, 8, 3, 4, 2, 9,\n",
      "        4, 9, 8, 1, 9, 2, 9, 7, 8, 1, 5, 5, 3, 3, 4, 5, 4, 9, 0, 4, 1, 3, 7, 6,\n",
      "        5, 5, 2, 7, 3, 3, 8, 6, 1, 3, 1, 9, 5, 5, 9, 1, 6, 5, 6, 8, 1, 5, 4, 2,\n",
      "        5, 8, 9, 7, 5, 4, 6, 0, 1, 0, 5, 4, 6, 8, 2, 2, 1, 4, 2, 5, 1, 5, 1, 1,\n",
      "        3, 8, 5, 9, 2, 9, 8, 8, 4, 4, 6, 1, 8, 1, 8, 0, 3, 2, 4, 6, 3, 9, 6, 2,\n",
      "        9, 1, 3, 4, 1, 4, 0, 7, 4, 4, 2, 0, 2, 1, 4, 8, 4, 0, 5, 0, 7, 6, 3, 5,\n",
      "        9, 4, 3, 4, 5, 7, 3, 7, 8, 3, 3, 1, 5, 7, 7, 6, 2, 0, 0, 4, 1, 2, 1, 7,\n",
      "        5, 1, 2, 2, 2, 6, 5, 9, 5, 9, 3, 2, 8, 9, 0, 7, 9, 2, 2, 8, 3, 1, 6, 7,\n",
      "        0, 1, 4, 7, 1, 1, 0, 6, 1, 5, 4, 5, 3, 2, 6, 6, 5, 4, 8, 1, 9, 6, 1, 1,\n",
      "        0, 3, 8, 5, 8, 3, 3, 8, 7, 4, 0, 6, 9, 8, 3, 8, 5, 9, 0, 6, 9, 7, 6, 6,\n",
      "        2, 0, 9, 9, 5, 1, 1, 3, 9, 9, 9, 5, 7, 3, 5, 1, 3, 3, 9, 4, 1, 5, 9, 5,\n",
      "        8, 9, 8, 7, 2, 5, 6, 0, 8, 5, 8, 6, 5, 0, 2, 9, 7, 0, 2, 0, 0, 4, 7, 2,\n",
      "        1, 5, 2, 9, 2, 8, 2, 9, 0, 4, 2, 0, 9, 2, 2, 4, 3, 2, 7, 2, 9, 2, 7, 9,\n",
      "        8, 5, 0, 7, 7, 9, 8, 5, 9, 7, 3, 4, 9, 1, 7, 0, 2, 3, 4, 7, 4, 7, 6, 4,\n",
      "        5, 8, 4, 1, 6, 1, 4, 0, 9, 1, 5, 5, 0, 5, 3, 1, 0, 2, 6, 2, 6, 8, 6, 4,\n",
      "        9, 6, 2, 2, 3, 4, 7, 8, 4, 7, 4, 3, 6, 4, 4, 7, 1, 6, 5, 9, 7, 6, 9, 3,\n",
      "        2, 5, 1, 9, 7, 8, 8, 0, 4, 6, 2, 4, 7, 8, 3, 3, 6, 5, 6, 9, 7, 3, 0, 7,\n",
      "        4, 3, 9, 1, 3, 9, 0, 5, 5, 8, 3, 9, 9, 6, 2, 3, 3, 2, 0, 0, 0, 1, 8, 4,\n",
      "        8, 6, 1, 2, 1, 2, 0, 9, 5, 1, 9, 9, 6, 1, 8, 5, 3, 9, 7, 4, 5, 3, 6, 5,\n",
      "        5, 8, 8, 0, 8, 7, 1, 1, 7, 6, 3, 0, 9, 1, 3, 6, 1, 0, 6, 9, 6, 5, 9, 9,\n",
      "        5, 1, 3, 5, 7, 3, 1, 9, 7, 9, 4, 6, 2, 5, 0, 1, 8, 0, 5, 6, 7, 7, 6, 7,\n",
      "        4, 8, 0, 2, 9, 7, 1, 3, 5, 3, 4, 7, 6, 4, 2, 5, 7, 1, 6, 2, 4, 0, 1, 1,\n",
      "        9, 1, 2, 8, 0, 0, 0, 9, 8, 6, 9, 6])\n",
      "0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logtis = seq(X_test_t)\n",
    "    perdiction = logtis.argmax(dim=1)\n",
    "    print(perdiction)\n",
    "    acc = (perdiction == y_test_t).float().mean().item()\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "497378ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e47b7",
   "metadata": {},
   "source": [
    "working with nn module as child-parent where mlp is chile of nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3e37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input=64,hid=100,num_class = 10):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input,hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(hid,num_class)\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.model(X)\n",
    "\n",
    "\n",
    "mlp_model = MLP()\n",
    "\n",
    "\n",
    "\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc80ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257\n",
      "epoch 0 | loss is 2.277\n",
      "epoch 5 | loss is 2.279\n",
      "epoch 10 | loss is 2.213\n",
      "epoch 15 | loss is 2.217\n",
      "epoch 20 | loss is 2.205\n",
      "epoch 25 | loss is 2.160\n",
      "epoch 30 | loss is 2.068\n",
      "epoch 35 | loss is 2.100\n",
      "epoch 40 | loss is 2.043\n",
      "epoch 45 | loss is 2.011\n",
      "epoch 50 | loss is 2.009\n",
      "epoch 55 | loss is 1.929\n",
      "epoch 60 | loss is 1.901\n",
      "epoch 65 | loss is 1.875\n",
      "epoch 70 | loss is 1.768\n",
      "epoch 75 | loss is 1.681\n",
      "epoch 80 | loss is 1.654\n",
      "epoch 85 | loss is 1.701\n",
      "epoch 90 | loss is 1.530\n",
      "epoch 95 | loss is 1.598\n",
      "epoch 100 | loss is 1.479\n",
      "epoch 105 | loss is 1.464\n",
      "epoch 110 | loss is 1.424\n",
      "epoch 115 | loss is 1.436\n",
      "epoch 120 | loss is 1.246\n",
      "epoch 125 | loss is 1.163\n",
      "epoch 130 | loss is 1.183\n",
      "epoch 135 | loss is 1.011\n",
      "epoch 140 | loss is 1.122\n",
      "epoch 145 | loss is 0.928\n",
      "epoch 150 | loss is 0.924\n",
      "epoch 155 | loss is 1.040\n",
      "epoch 160 | loss is 0.920\n",
      "epoch 165 | loss is 0.970\n",
      "epoch 170 | loss is 0.850\n",
      "epoch 175 | loss is 0.858\n",
      "epoch 180 | loss is 0.720\n",
      "epoch 185 | loss is 0.846\n",
      "epoch 190 | loss is 0.660\n",
      "epoch 195 | loss is 0.746\n",
      "epoch 200 | loss is 0.583\n",
      "epoch 205 | loss is 0.801\n",
      "epoch 210 | loss is 0.692\n",
      "epoch 215 | loss is 0.703\n",
      "epoch 220 | loss is 0.725\n",
      "epoch 225 | loss is 0.547\n",
      "epoch 230 | loss is 0.377\n",
      "epoch 235 | loss is 0.624\n",
      "epoch 240 | loss is 0.654\n",
      "epoch 245 | loss is 0.581\n",
      "epoch 250 | loss is 0.403\n",
      "epoch 255 | loss is 0.585\n",
      "epoch 260 | loss is 0.518\n",
      "epoch 265 | loss is 0.384\n",
      "epoch 270 | loss is 0.418\n",
      "epoch 275 | loss is 0.392\n",
      "epoch 280 | loss is 0.360\n",
      "epoch 285 | loss is 0.462\n",
      "epoch 290 | loss is 0.321\n",
      "epoch 295 | loss is 0.641\n",
      "epoch 300 | loss is 0.274\n",
      "epoch 305 | loss is 0.338\n",
      "epoch 310 | loss is 0.404\n",
      "epoch 315 | loss is 0.398\n",
      "epoch 320 | loss is 0.280\n",
      "epoch 325 | loss is 0.295\n",
      "epoch 330 | loss is 0.368\n",
      "epoch 335 | loss is 0.418\n",
      "epoch 340 | loss is 0.320\n",
      "epoch 345 | loss is 0.465\n",
      "epoch 350 | loss is 0.492\n",
      "epoch 355 | loss is 0.343\n",
      "epoch 360 | loss is 0.201\n",
      "epoch 365 | loss is 0.267\n",
      "epoch 370 | loss is 0.220\n",
      "epoch 375 | loss is 0.283\n",
      "epoch 380 | loss is 0.252\n",
      "epoch 385 | loss is 0.465\n",
      "epoch 390 | loss is 0.359\n",
      "epoch 395 | loss is 0.450\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer now with L2 regularization \n",
    "optimizer = optim.SGD(mlp_model.parameters(),lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 400\n",
    "nusamples = X_train_t.shape[0]\n",
    "print(nusamples)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(nusamples)\n",
    "\n",
    "    for i in range(0,nusamples,batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_t[idx]\n",
    "\n",
    "        logits = mlp_model(X_batch)\n",
    "        loss = criterion(logits,y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch} | loss is {loss.item():.3f}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423adf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9277777671813965\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = mlp_model(X_test_t)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == y_test_t).float().mean()\n",
    "\n",
    "print(\"\\nTest accuracy:\", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e06ea4",
   "metadata": {},
   "source": [
    "SGD With Momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e83fcc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input=64,hid=100,num_class = 10,drop_out=0.3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input,hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(hid,num_class)\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.model(X)\n",
    "\n",
    "\n",
    "mlp_model = MLP()\n",
    "\n",
    "\n",
    "\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f70f5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, X_train_t, y_train_t, batch_size=20, epochs=400):\n",
    "    nusamples = X_train_t.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        perm = torch.randperm(nusamples)\n",
    "\n",
    "        for i in range(0, nusamples, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "\n",
    "            X_batch = X_train_t[idx]\n",
    "            y_batch = y_train_t[idx]\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"epoch {epoch} | loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "695c6d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | loss = 2.2342\n",
      "epoch 5 | loss = 1.9547\n",
      "epoch 10 | loss = 1.4580\n",
      "epoch 15 | loss = 1.0414\n",
      "epoch 20 | loss = 0.7392\n",
      "epoch 25 | loss = 0.4108\n",
      "epoch 30 | loss = 0.3742\n",
      "epoch 35 | loss = 0.3238\n",
      "epoch 40 | loss = 0.2192\n",
      "epoch 45 | loss = 0.2553\n",
      "epoch 50 | loss = 0.3319\n",
      "epoch 55 | loss = 0.2566\n",
      "epoch 60 | loss = 0.1744\n",
      "epoch 65 | loss = 0.1980\n",
      "epoch 70 | loss = 0.0811\n",
      "epoch 75 | loss = 0.3154\n",
      "epoch 80 | loss = 0.1735\n",
      "epoch 85 | loss = 0.3877\n",
      "epoch 90 | loss = 0.1307\n",
      "epoch 95 | loss = 0.1109\n",
      "epoch 100 | loss = 0.1181\n",
      "epoch 105 | loss = 0.2061\n",
      "epoch 110 | loss = 0.1133\n",
      "epoch 115 | loss = 0.0871\n",
      "epoch 120 | loss = 0.0440\n",
      "epoch 125 | loss = 0.0777\n",
      "epoch 130 | loss = 0.0252\n",
      "epoch 135 | loss = 0.1786\n",
      "epoch 140 | loss = 0.1729\n",
      "epoch 145 | loss = 0.0920\n",
      "epoch 150 | loss = 0.0269\n",
      "epoch 155 | loss = 0.0439\n",
      "epoch 160 | loss = 0.1896\n",
      "epoch 165 | loss = 0.0495\n",
      "epoch 170 | loss = 0.0164\n",
      "epoch 175 | loss = 0.0315\n",
      "epoch 180 | loss = 0.0682\n",
      "epoch 185 | loss = 0.0808\n",
      "epoch 190 | loss = 0.1552\n",
      "epoch 195 | loss = 0.1060\n",
      "epoch 200 | loss = 0.0972\n",
      "epoch 205 | loss = 0.0398\n",
      "epoch 210 | loss = 0.0583\n",
      "epoch 215 | loss = 0.0464\n",
      "epoch 220 | loss = 0.0858\n",
      "epoch 225 | loss = 0.0994\n",
      "epoch 230 | loss = 0.0556\n",
      "epoch 235 | loss = 0.0689\n",
      "epoch 240 | loss = 0.0520\n",
      "epoch 245 | loss = 0.0584\n",
      "epoch 250 | loss = 0.0396\n",
      "epoch 255 | loss = 0.0355\n",
      "epoch 260 | loss = 0.0392\n",
      "epoch 265 | loss = 0.0408\n",
      "epoch 270 | loss = 0.1044\n",
      "epoch 275 | loss = 0.2338\n",
      "epoch 280 | loss = 0.0172\n",
      "epoch 285 | loss = 0.0757\n",
      "epoch 290 | loss = 0.1066\n",
      "epoch 295 | loss = 0.0558\n",
      "epoch 300 | loss = 0.0405\n",
      "epoch 305 | loss = 0.0263\n",
      "epoch 310 | loss = 0.0135\n",
      "epoch 315 | loss = 0.3131\n",
      "epoch 320 | loss = 0.0110\n",
      "epoch 325 | loss = 0.0983\n",
      "epoch 330 | loss = 0.0516\n",
      "epoch 335 | loss = 0.0545\n",
      "epoch 340 | loss = 0.1840\n",
      "epoch 345 | loss = 0.0119\n",
      "epoch 350 | loss = 0.1760\n",
      "epoch 355 | loss = 0.0755\n",
      "epoch 360 | loss = 0.0480\n",
      "epoch 365 | loss = 0.0307\n",
      "epoch 370 | loss = 0.0336\n",
      "epoch 375 | loss = 0.0850\n",
      "epoch 380 | loss = 0.0210\n",
      "epoch 385 | loss = 0.0976\n",
      "epoch 390 | loss = 0.0291\n",
      "epoch 395 | loss = 0.0291\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(drop_out=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    mlp_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    momentum=0.9,       \n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "train_model(mlp_model, optimizer, criterion, X_train_t, y_train_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5089cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = mlp_model(X_test_t)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == y_test_t).float().mean()\n",
    "\n",
    "print(\"\\nTest accuracy:\", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99cc298",
   "metadata": {},
   "source": [
    "model with AdaGrad optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb424d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | loss = 1.1764\n",
      "epoch 5 | loss = 0.3462\n",
      "epoch 10 | loss = 0.3653\n",
      "epoch 15 | loss = 0.2346\n",
      "epoch 20 | loss = 0.3359\n",
      "epoch 25 | loss = 0.3891\n",
      "epoch 30 | loss = 0.4794\n",
      "epoch 35 | loss = 0.0948\n",
      "epoch 40 | loss = 0.2572\n",
      "epoch 45 | loss = 0.0591\n",
      "epoch 50 | loss = 0.0988\n",
      "epoch 55 | loss = 0.0368\n",
      "epoch 60 | loss = 0.0573\n",
      "epoch 65 | loss = 0.0952\n",
      "epoch 70 | loss = 0.0499\n",
      "epoch 75 | loss = 0.1612\n",
      "epoch 80 | loss = 0.0828\n",
      "epoch 85 | loss = 0.0907\n",
      "epoch 90 | loss = 0.0596\n",
      "epoch 95 | loss = 0.0538\n",
      "epoch 100 | loss = 0.0915\n",
      "epoch 105 | loss = 0.0263\n",
      "epoch 110 | loss = 0.0443\n",
      "epoch 115 | loss = 0.0920\n",
      "epoch 120 | loss = 0.0534\n",
      "epoch 125 | loss = 0.0781\n",
      "epoch 130 | loss = 0.1024\n",
      "epoch 135 | loss = 0.1253\n",
      "epoch 140 | loss = 0.0395\n",
      "epoch 145 | loss = 0.0762\n",
      "epoch 150 | loss = 0.0214\n",
      "epoch 155 | loss = 0.0759\n",
      "epoch 160 | loss = 0.0992\n",
      "epoch 165 | loss = 0.0808\n",
      "epoch 170 | loss = 0.0470\n",
      "epoch 175 | loss = 0.1142\n",
      "epoch 180 | loss = 0.0847\n",
      "epoch 185 | loss = 0.0241\n",
      "epoch 190 | loss = 0.0468\n",
      "epoch 195 | loss = 0.0846\n",
      "epoch 200 | loss = 0.1665\n",
      "epoch 205 | loss = 0.0190\n",
      "epoch 210 | loss = 0.1054\n",
      "epoch 215 | loss = 0.1189\n",
      "epoch 220 | loss = 0.0288\n",
      "epoch 225 | loss = 0.0850\n",
      "epoch 230 | loss = 0.1879\n",
      "epoch 235 | loss = 0.2566\n",
      "epoch 240 | loss = 0.0698\n",
      "epoch 245 | loss = 0.0401\n",
      "epoch 250 | loss = 0.0429\n",
      "epoch 255 | loss = 0.0161\n",
      "epoch 260 | loss = 0.0299\n",
      "epoch 265 | loss = 0.4070\n",
      "epoch 270 | loss = 0.0437\n",
      "epoch 275 | loss = 0.0663\n",
      "epoch 280 | loss = 0.0164\n",
      "epoch 285 | loss = 0.2014\n",
      "epoch 290 | loss = 0.0335\n",
      "epoch 295 | loss = 0.1002\n",
      "epoch 300 | loss = 0.0167\n",
      "epoch 305 | loss = 0.1112\n",
      "epoch 310 | loss = 0.0549\n",
      "epoch 315 | loss = 0.0033\n",
      "epoch 320 | loss = 0.0558\n",
      "epoch 325 | loss = 0.0126\n",
      "epoch 330 | loss = 0.0789\n",
      "epoch 335 | loss = 0.0853\n",
      "epoch 340 | loss = 0.0243\n",
      "epoch 345 | loss = 0.0125\n",
      "epoch 350 | loss = 0.0060\n",
      "epoch 355 | loss = 0.0269\n",
      "epoch 360 | loss = 0.0900\n",
      "epoch 365 | loss = 0.0104\n",
      "epoch 370 | loss = 0.0087\n",
      "epoch 375 | loss = 0.0337\n",
      "epoch 380 | loss = 0.0675\n",
      "epoch 385 | loss = 0.0107\n",
      "epoch 390 | loss = 0.0307\n",
      "epoch 395 | loss = 0.0191\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(drop_out=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adagrad(\n",
    "    mlp_model.parameters(),\n",
    "    lr=1e-2,         # <-- Adagrad usually needs larger LR\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "train_model(mlp_model, optimizer, criterion, X_train_t, y_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3c4c538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9685184955596924\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = mlp_model(X_test_t)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == y_test_t).float().mean()\n",
    "\n",
    "print(\"\\nTest accuracy:\", accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f01df",
   "metadata": {},
   "source": [
    "Now optimization Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d81c0841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | loss = 1.9017\n",
      "epoch 5 | loss = 0.4206\n",
      "epoch 10 | loss = 0.3083\n",
      "epoch 15 | loss = 0.1387\n",
      "epoch 20 | loss = 0.0752\n",
      "epoch 25 | loss = 0.0547\n",
      "epoch 30 | loss = 0.0719\n",
      "epoch 35 | loss = 0.0902\n",
      "epoch 40 | loss = 0.0182\n",
      "epoch 45 | loss = 0.0289\n",
      "epoch 50 | loss = 0.1450\n",
      "epoch 55 | loss = 0.0185\n",
      "epoch 60 | loss = 0.0595\n",
      "epoch 65 | loss = 0.0213\n",
      "epoch 70 | loss = 0.0497\n",
      "epoch 75 | loss = 0.0036\n",
      "epoch 80 | loss = 0.1566\n",
      "epoch 85 | loss = 0.0200\n",
      "epoch 90 | loss = 0.0087\n",
      "epoch 95 | loss = 0.0256\n",
      "epoch 100 | loss = 0.0060\n",
      "epoch 105 | loss = 0.0026\n",
      "epoch 110 | loss = 0.0294\n",
      "epoch 115 | loss = 0.0089\n",
      "epoch 120 | loss = 0.0066\n",
      "epoch 125 | loss = 0.0029\n",
      "epoch 130 | loss = 0.0162\n",
      "epoch 135 | loss = 0.0068\n",
      "epoch 140 | loss = 0.0043\n",
      "epoch 145 | loss = 0.0049\n",
      "epoch 150 | loss = 0.0050\n",
      "epoch 155 | loss = 0.0069\n",
      "epoch 160 | loss = 0.0471\n",
      "epoch 165 | loss = 0.0043\n",
      "epoch 170 | loss = 0.0014\n",
      "epoch 175 | loss = 0.0080\n",
      "epoch 180 | loss = 0.0020\n",
      "epoch 185 | loss = 0.0078\n",
      "epoch 190 | loss = 0.0272\n",
      "epoch 195 | loss = 0.0028\n",
      "epoch 200 | loss = 0.0045\n",
      "epoch 205 | loss = 0.0063\n",
      "epoch 210 | loss = 0.0031\n",
      "epoch 215 | loss = 0.0058\n",
      "epoch 220 | loss = 0.0026\n",
      "epoch 225 | loss = 0.1105\n",
      "epoch 230 | loss = 0.0065\n",
      "epoch 235 | loss = 0.0096\n",
      "epoch 240 | loss = 0.0104\n",
      "epoch 245 | loss = 0.0054\n",
      "epoch 250 | loss = 0.0044\n",
      "epoch 255 | loss = 0.0058\n",
      "epoch 260 | loss = 0.0097\n",
      "epoch 265 | loss = 0.0067\n",
      "epoch 270 | loss = 0.0053\n",
      "epoch 275 | loss = 0.0109\n",
      "epoch 280 | loss = 0.0101\n",
      "epoch 285 | loss = 0.0022\n",
      "epoch 290 | loss = 0.0015\n",
      "epoch 295 | loss = 0.0022\n",
      "epoch 300 | loss = 0.0047\n",
      "epoch 305 | loss = 0.0036\n",
      "epoch 310 | loss = 0.0026\n",
      "epoch 315 | loss = 0.0061\n",
      "epoch 320 | loss = 0.0006\n",
      "epoch 325 | loss = 0.0011\n",
      "epoch 330 | loss = 0.0009\n",
      "epoch 335 | loss = 0.0029\n",
      "epoch 340 | loss = 0.0945\n",
      "epoch 345 | loss = 0.0533\n",
      "epoch 350 | loss = 0.0009\n",
      "epoch 355 | loss = 0.0002\n",
      "epoch 360 | loss = 0.0264\n",
      "epoch 365 | loss = 0.0021\n",
      "epoch 370 | loss = 0.0042\n",
      "epoch 375 | loss = 0.0007\n",
      "epoch 380 | loss = 0.0051\n",
      "epoch 385 | loss = 0.0009\n",
      "epoch 390 | loss = 0.0022\n",
      "epoch 395 | loss = 0.0006\n",
      "\n",
      "Test accuracy: 0.9777777791023254\n"
     ]
    }
   ],
   "source": [
    "mlp_model = MLP(drop_out=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    mlp_model.parameters(),\n",
    "    lr=1e-3,        # good default\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "train_model(mlp_model, optimizer, criterion, X_train_t, y_train_t)\n",
    "\n",
    "\n",
    "\n",
    "#print the final accurcy of the model\n",
    "with torch.no_grad():\n",
    "    logits = mlp_model(X_test_t)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == y_test_t).float().mean()\n",
    "\n",
    "print(\"\\nTest accuracy:\", accuracy.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
