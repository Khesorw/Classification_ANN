{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121cc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfaf3076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(3,4)\n",
    "\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e93bebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self,input_dim,num_neurons):\n",
    "    \n",
    "        self.w1 = torch.rand(input_dim,num_neurons,requires_grad=True)\n",
    "        self.w2 = torch.rand(num_neurons,1,requires_grad=True)\n",
    "        self.Loss = []\n",
    "        self.b1 = torch.zeros(1,num_neurons,requires_grad=True)\n",
    "        self.b2 = torch.zeros(1,1,requires_grad=True)\n",
    "\n",
    "    \n",
    "    def Tanh(self,x):\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def ReLU(self,x):\n",
    "        return torch.relu(x)\n",
    "    \n",
    "\n",
    "    def softmax(self,x):\n",
    "        ex = torch.exp(x - x.max(dim=1,keepdim=True).values)\n",
    "        return ex / ex.sum(dim=1,keepdim=True)\n",
    "\n",
    "    def cross_entropy(self,perdicted,target): \n",
    "        per_sample = - torch.sum(target * torch.log(perdicted + 1e-12),dim=1) \n",
    "        return per_sample.mean()\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        self.z1 = X @ self.w1 + self.b1\n",
    "\n",
    "        self.h1 = self.ReLU(self.z1)\n",
    "\n",
    "        self.z2 = self.h1 @ self.w2 + self.b2\n",
    "\n",
    "        self.y_hat = self.softmax(self.z2)\n",
    "\n",
    "        return self.y_hat\n",
    "    \n",
    "\n",
    "\n",
    "    def back_prop(self,target,lr=1e-3):\n",
    "        loss_value =  self.cross_entropy(self.y_hat,target=target)\n",
    "        self.Loss.append(loss_value.item())\n",
    "\n",
    "        #clearing old gradient good practice\n",
    "        if self.w1.grad is not None:\n",
    "            self.w1.grad.zero_()\n",
    "            self.b1.grad.zero_()\n",
    "            self.w2.grad.zero_()\n",
    "            self.b2.grad.zero_()\n",
    "\n",
    "        #clac the gradient\n",
    "\n",
    "        loss_value.backward()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.w1 -= lr * self.w1.grad\n",
    "            self.b1 -= lr * self.b1.grad\n",
    "\n",
    "            self.w2 -= lr * self.w2.grad\n",
    "            self.b2 -= lr * self.b2.grad\n",
    "\n",
    "        #after calling torch.no_grad the gradient should be re-enabled\n",
    "        self.w1.requires_grad_(True)\n",
    "        self.b1.requires_grad_(True)\n",
    "        self.w2.requires_grad_(True)\n",
    "        self.b2.requires_grad_(True)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, num_neurons):\n",
    "\n",
    "        # MAKE WEIGHTS LEAF TENSORS \n",
    "        self.w1 = torch.randn(input_dim, num_neurons, requires_grad=True)\n",
    "        self.w1.data *= 0.01\n",
    "\n",
    "        self.b1 = torch.zeros(1, num_neurons, requires_grad=True)\n",
    "\n",
    "        self.w2 = torch.randn(num_neurons, 10, requires_grad=True)\n",
    "        self.w2.data *= 0.01\n",
    "\n",
    "        self.b2 = torch.zeros(1, 10, requires_grad=True)\n",
    "\n",
    "        self.Loss = []\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return torch.relu(x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        ex = torch.exp(x - x.max(dim=1, keepdim=True).values)\n",
    "        return ex / ex.sum(dim=1, keepdim=True)\n",
    "\n",
    "    def cross_entropy(self, predicted, target):\n",
    "        per_sample = -torch.sum(target * torch.log(predicted + 1e-12), dim=1)\n",
    "        return per_sample.mean()\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.w1 + self.b1\n",
    "        self.h1 = self.ReLU(self.z1)\n",
    "        self.z2 = self.h1 @ self.w2 + self.b2\n",
    "        self.y_hat = self.softmax(self.z2)\n",
    "        return self.y_hat\n",
    "\n",
    "    def back_prop(self, target, lr=1e-3):\n",
    "\n",
    "        loss = self.cross_entropy(self.y_hat, target)\n",
    "        self.Loss.append(float(loss.detach()))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.w1 -= lr * self.w1.grad\n",
    "            self.b1 -= lr * self.b1.grad\n",
    "            self.w2 -= lr * self.w2.grad\n",
    "            self.b2 -= lr * self.b2.grad\n",
    "\n",
    "        self.w1.grad.zero_()\n",
    "        self.b1.grad.zero_()\n",
    "        self.w2.grad.zero_()\n",
    "        self.b2.grad.zero_()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9f70e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(input_dim=64, num_neurons=100)\n",
    "\n",
    "print(model.w1.grad, model.w2.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "40acb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 2.3040\n",
      "Epoch 5, Loss = 2.3028\n",
      "Epoch 10, Loss = 2.2999\n",
      "Epoch 15, Loss = 2.3015\n",
      "Epoch 20, Loss = 2.3036\n",
      "Epoch 25, Loss = 2.2999\n",
      "Epoch 30, Loss = 2.2991\n",
      "Epoch 35, Loss = 2.2990\n",
      "Epoch 40, Loss = 2.3016\n",
      "Epoch 45, Loss = 2.3010\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm  = scaler.transform(X_test)  # FIXED\n",
    "\n",
    "# Tensors\n",
    "X_train_t = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_norm, dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# One-hot\n",
    "y_train_oh = torch.eye(10)[y_train_t]\n",
    "y_test_oh  = torch.eye(10)[y_test_t]\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_oh[idx]\n",
    "\n",
    "        model.forward(X_batch)\n",
    "        loss = model.back_prop(y_batch, lr=lr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1797, 64)\n",
      "y shape: (1797,)\n",
      "Train: (1257, 64) | Test: (540, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data        # shape (1797, 64)\n",
    "y = digits.target      # labels 0â€“9\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d4540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 262 1277 1133 1284 1557 1131 1281  626   49  449]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "idx = np.random.choice(len(X), 10, replace=False)\n",
    "print(idx)\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "for i, img_idx in enumerate(idx):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(X[img_idx].reshape(-1,8), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {y[img_idx]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08b93c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.float32\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_norm = scaler.fit_transform(X_train)\n",
    "x_test_norm = scaler.fit_transform(X_test)\n",
    "\n",
    "#converting them to tensor\n",
    "\n",
    "X_train_t = torch.tensor(x_train_norm,dtype=torch.float32)\n",
    "X_test_t = torch.tensor(x_test_norm,dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train,dtype=torch.long)\n",
    "y_test_t = torch.tensor(y_test,dtype=torch.long)\n",
    "\n",
    "\n",
    "print(y_test_t.dtype,x_test_t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "967f7b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def onehot(label):\n",
    "    return torch.eye(10)[label]\n",
    "\n",
    "y_train_oh = onehot(y_train_t)\n",
    "y_test_oh  = onehot(y_test_t)\n",
    "print(y_test_oh[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ad59ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_dim=64, num_neurons=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84205805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 0.0000\n",
      "Epoch 5, Loss = 0.0000\n",
      "Epoch 10, Loss = 0.0000\n",
      "Epoch 15, Loss = 0.0000\n",
      "Epoch 20, Loss = 0.0000\n",
      "Epoch 25, Loss = 0.0000\n",
      "Epoch 30, Loss = 0.0000\n",
      "Epoch 35, Loss = 0.0000\n",
      "Epoch 40, Loss = 0.0000\n",
      "Epoch 45, Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    " \n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i : i + batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_oh[idx]\n",
    "\n",
    "       \n",
    "        y_pred = model.forward(X_batch)\n",
    "\n",
    " \n",
    "        model.back_prop(y_batch, lr=lr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {model.Loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c492390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b82a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, num_neurons, num_classes=10):\n",
    "        #nn.Parameter when initializing\n",
    "        self.w1 = torch.randn(input_dim, num_neurons) * 0.01\n",
    "        self.w1 = torch.nn.Parameter(self.w1)\n",
    "\n",
    "        self.b1 = torch.zeros(1, num_neurons)\n",
    "        self.b1 = torch.nn.Parameter(self.b1)\n",
    "\n",
    "        self.w2 = torch.randn(num_neurons, num_classes) * 0.01\n",
    "        self.w2 = torch.nn.Parameter(self.w2)\n",
    "\n",
    "        self.b2 = torch.zeros(1, num_classes)\n",
    "        self.b2 = torch.nn.Parameter(self.b2)\n",
    "\n",
    "        self.Loss = []\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return torch.relu(x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        ex = torch.exp(x - x.max(dim=1, keepdim=True).values)\n",
    "        return ex / ex.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #loss cross-entropy \n",
    "    def cross_entropy(self, predicted, target):\n",
    "        per_sample = -torch.sum(target * torch.log(predicted + 1e-12), dim=1)\n",
    "        return per_sample.mean()\n",
    "\n",
    "    # ---- Forward ----\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.w1 + self.b1\n",
    "        self.h1 = self.ReLU(self.z1)\n",
    "        self.z2 = self.h1 @ self.w2 + self.b2\n",
    "        self.y_hat = self.softmax(self.z2)\n",
    "        return self.y_hat\n",
    "\n",
    "    # ---- Backward ----\n",
    "    def back_prop(self, target, lr=1e-3):\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.cross_entropy(self.y_hat, target)\n",
    "        self.Loss.append(loss.item())\n",
    "\n",
    "        # clear old gradients\n",
    "        for p in [self.w1, self.b1, self.w2, self.b2]:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "     \n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.w1 -= lr * self.w1.grad\n",
    "            self.b1 -= lr * self.b1.grad\n",
    "            self.w2 -= lr * self.w2.grad\n",
    "            self.b2 -= lr * self.b2.grad\n",
    "\n",
    "        # re-enable grad\n",
    "        self.w1.requires_grad_(True)\n",
    "        self.b1.requires_grad_(True)\n",
    "        self.w2.requires_grad_(True)\n",
    "        self.b2.requires_grad_(True)\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdb4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=23\n",
    ")\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "# convert to tensors\n",
    "X_train_t = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_norm, dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# one-hot\n",
    "y_train_oh = torch.eye(10)[y_train_t]\n",
    "y_test_oh  = torch.eye(10)[y_test_t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70234fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100]) \n",
      "\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(input_dim=64, num_neurons=100, num_classes=10)\n",
    "print(model.w1.shape,'\\n')\n",
    "print(model.w2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 1.0875\n",
      "Epoch 5, Loss = 1.0860\n",
      "Epoch 10, Loss = 0.5233\n",
      "Epoch 15, Loss = 0.5310\n",
      "Epoch 20, Loss = 0.7772\n",
      "Epoch 25, Loss = 0.3361\n",
      "Epoch 30, Loss = 0.6081\n",
      "Epoch 35, Loss = 0.3299\n",
      "Epoch 40, Loss = 0.3692\n",
      "Epoch 45, Loss = 0.3623\n",
      "Epoch 50, Loss = 0.2321\n",
      "Epoch 55, Loss = 0.4991\n",
      "Epoch 60, Loss = 0.3650\n",
      "Epoch 65, Loss = 0.1187\n",
      "Epoch 70, Loss = 0.2927\n",
      "Epoch 75, Loss = 0.3862\n",
      "Epoch 80, Loss = 0.1684\n",
      "Epoch 85, Loss = 0.1233\n",
      "Epoch 90, Loss = 0.0827\n",
      "Epoch 95, Loss = 0.1249\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lr = 0.01          \n",
    "epochs = 100\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_oh[idx]\n",
    "\n",
    "        model.forward(X_batch)\n",
    "        loss = model.back_prop(y_batch, lr=lr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1edfd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9259259104728699\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.forward(X_test_t)\n",
    "pred_labels = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "acc = (pred_labels == y_test_t).float().mean().item()\n",
    "print(\"Test Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3eb5a",
   "metadata": {},
   "source": [
    "Now classification with nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f6aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "seq = nn.Sequential(\n",
    "    nn.Linear(64,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,10)\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(seq.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = nn.Sequential(\n",
    "    nn.Linear(64,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,10)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(seq.parameters(),lr=1e-3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924048e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape of perm torch.Size([1257]) now permd \n",
      "tensor([782, 911, 674,  ..., 358,   9, 812])\n",
      "1257\n",
      "\n",
      "Final Test Accuracy: 0.9722222089767456\n"
     ]
    }
   ],
   "source": [
    "def test_step():\n",
    "    seq.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = seq(X_test_t)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        accuracy = (predictions == y_test_t).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Training loop (functional)\n",
    "# ---------------------------------------------------\n",
    "epochs = 500\n",
    "batch_size = 5\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(num_samples)\n",
    "    print(f' shape of perm {perm.shape} now permd \\n{perm}\\n{num_samples}')\n",
    "    \n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_t[idx]\n",
    "        seq.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = seq(X_batch)\n",
    "        loss = criterion(logits,y_batch) #ce losss\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lo = loss.item()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        acc = test_step()\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {lo:.4f} | Test Acc: {acc:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Final accuracy\n",
    "# ---------------------------------------------------\n",
    "final_acc = test_step()\n",
    "print(\"\\nFinal Test Accuracy:\", final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5219f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 9, 3, 7, 2, 1, 5, 2, 5, 2, 1, 9, 4, 0, 4, 2, 3, 7, 8, 8, 4, 3, 9, 7,\n",
      "        5, 6, 3, 5, 6, 3, 4, 9, 1, 4, 4, 6, 9, 4, 7, 6, 6, 9, 1, 3, 6, 1, 3, 0,\n",
      "        6, 5, 5, 1, 9, 5, 6, 0, 9, 0, 0, 1, 0, 4, 5, 2, 4, 5, 7, 0, 7, 5, 9, 5,\n",
      "        5, 4, 7, 0, 4, 5, 5, 9, 9, 0, 2, 3, 8, 0, 6, 4, 4, 9, 1, 2, 8, 3, 5, 2,\n",
      "        9, 0, 4, 4, 4, 3, 5, 3, 1, 3, 5, 9, 4, 2, 7, 7, 4, 4, 1, 9, 2, 7, 8, 7,\n",
      "        2, 6, 9, 4, 0, 7, 2, 7, 5, 8, 7, 5, 7, 9, 0, 6, 6, 4, 2, 8, 0, 9, 4, 6,\n",
      "        9, 9, 6, 9, 0, 5, 5, 6, 6, 0, 6, 4, 3, 9, 3, 8, 7, 2, 9, 0, 4, 5, 3, 6,\n",
      "        5, 9, 9, 8, 4, 2, 1, 3, 7, 7, 2, 2, 3, 9, 8, 0, 3, 2, 2, 5, 6, 9, 9, 4,\n",
      "        1, 2, 4, 2, 3, 6, 4, 8, 5, 9, 5, 7, 1, 9, 4, 8, 1, 5, 4, 4, 9, 6, 1, 8,\n",
      "        6, 0, 4, 5, 2, 7, 4, 6, 4, 5, 6, 0, 3, 2, 3, 6, 7, 1, 9, 1, 4, 7, 6, 5,\n",
      "        8, 5, 5, 1, 0, 1, 8, 8, 9, 9, 7, 6, 2, 2, 2, 3, 4, 8, 8, 3, 6, 0, 9, 7,\n",
      "        7, 0, 1, 0, 4, 5, 1, 5, 3, 6, 0, 4, 1, 0, 0, 3, 6, 5, 9, 7, 3, 5, 5, 9,\n",
      "        9, 8, 5, 3, 3, 2, 0, 5, 8, 3, 4, 0, 2, 4, 6, 4, 3, 4, 5, 0, 5, 2, 1, 3,\n",
      "        1, 4, 1, 1, 7, 0, 1, 5, 2, 1, 2, 8, 7, 0, 6, 4, 8, 8, 5, 1, 8, 4, 5, 8,\n",
      "        7, 9, 8, 6, 0, 6, 2, 0, 7, 9, 8, 9, 5, 2, 7, 7, 1, 8, 7, 4, 3, 8, 3, 5,\n",
      "        6, 0, 0, 3, 0, 5, 0, 0, 4, 1, 2, 8, 4, 5, 9, 6, 3, 4, 8, 8, 4, 2, 3, 8,\n",
      "        9, 8, 8, 5, 0, 6, 3, 3, 7, 1, 6, 4, 1, 2, 1, 1, 6, 4, 7, 4, 8, 3, 4, 0,\n",
      "        5, 1, 9, 4, 5, 7, 6, 3, 7, 0, 5, 9, 7, 5, 9, 7, 4, 2, 1, 9, 0, 7, 5, 2,\n",
      "        3, 6, 3, 9, 6, 9, 5, 0, 1, 5, 5, 8, 3, 3, 6, 2, 6, 5, 4, 2, 0, 8, 7, 3,\n",
      "        7, 0, 2, 2, 3, 5, 8, 7, 3, 6, 5, 9, 9, 2, 5, 6, 3, 0, 7, 1, 1, 9, 6, 1,\n",
      "        1, 0, 0, 2, 9, 3, 9, 9, 3, 7, 7, 1, 3, 5, 4, 6, 1, 2, 1, 1, 8, 7, 6, 9,\n",
      "        2, 0, 4, 4, 8, 8, 7, 1, 3, 1, 7, 1, 8, 5, 1, 7, 0, 0, 2, 2, 6, 9, 4, 8,\n",
      "        9, 0, 6, 7, 7, 9, 5, 4, 7, 0, 7, 6])\n",
      "0.9722222089767456\n"
     ]
    }
   ],
   "source": [
    "# seq.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logtis = seq(X_test_t)\n",
    "    perdiction = logtis.argmax(dim=1)\n",
    "    print(perdiction)\n",
    "    acc = (perdiction == y_test_t).float().mean().item()\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e47b7",
   "metadata": {},
   "source": [
    "working with nn module as child-parent where mlp is chile of nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3e37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input=64,hid=100,num_class = 10):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input,hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid,num_class)\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.model(X)\n",
    "\n",
    "\n",
    "mlp_model = MLP()\n",
    "\n",
    "\n",
    "\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc80ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257\n",
      "epoch 0 | loss is 0.057\n",
      "epoch 5 | loss is 0.117\n",
      "epoch 10 | loss is 0.398\n",
      "epoch 15 | loss is 0.171\n",
      "epoch 20 | loss is 0.153\n",
      "epoch 25 | loss is 0.236\n",
      "epoch 30 | loss is 0.114\n",
      "epoch 35 | loss is 0.106\n",
      "epoch 40 | loss is 0.100\n",
      "epoch 45 | loss is 0.128\n",
      "epoch 50 | loss is 0.097\n",
      "epoch 55 | loss is 0.158\n",
      "epoch 60 | loss is 0.146\n",
      "epoch 65 | loss is 0.117\n",
      "epoch 70 | loss is 0.080\n",
      "epoch 75 | loss is 0.124\n",
      "epoch 80 | loss is 0.065\n",
      "epoch 85 | loss is 0.132\n",
      "epoch 90 | loss is 0.159\n",
      "epoch 95 | loss is 0.152\n",
      "epoch 100 | loss is 0.152\n",
      "epoch 105 | loss is 0.042\n",
      "epoch 110 | loss is 0.171\n",
      "epoch 115 | loss is 0.164\n",
      "epoch 120 | loss is 0.060\n",
      "epoch 125 | loss is 0.259\n",
      "epoch 130 | loss is 0.135\n",
      "epoch 135 | loss is 0.061\n",
      "epoch 140 | loss is 0.029\n",
      "epoch 145 | loss is 0.205\n",
      "epoch 150 | loss is 0.073\n",
      "epoch 155 | loss is 0.166\n",
      "epoch 160 | loss is 0.175\n",
      "epoch 165 | loss is 0.357\n",
      "epoch 170 | loss is 0.081\n",
      "epoch 175 | loss is 0.176\n",
      "epoch 180 | loss is 0.091\n",
      "epoch 185 | loss is 0.029\n",
      "epoch 190 | loss is 0.088\n",
      "epoch 195 | loss is 0.195\n",
      "epoch 200 | loss is 0.074\n",
      "epoch 205 | loss is 0.082\n",
      "epoch 210 | loss is 0.115\n",
      "epoch 215 | loss is 0.086\n",
      "epoch 220 | loss is 0.144\n",
      "epoch 225 | loss is 0.056\n",
      "epoch 230 | loss is 0.031\n",
      "epoch 235 | loss is 0.141\n",
      "epoch 240 | loss is 0.023\n",
      "epoch 245 | loss is 0.099\n",
      "epoch 250 | loss is 0.274\n",
      "epoch 255 | loss is 0.146\n",
      "epoch 260 | loss is 0.040\n",
      "epoch 265 | loss is 0.051\n",
      "epoch 270 | loss is 0.052\n",
      "epoch 275 | loss is 0.092\n",
      "epoch 280 | loss is 0.263\n",
      "epoch 285 | loss is 0.074\n",
      "epoch 290 | loss is 0.269\n",
      "epoch 295 | loss is 0.139\n",
      "epoch 300 | loss is 0.184\n",
      "epoch 305 | loss is 0.131\n",
      "epoch 310 | loss is 0.102\n",
      "epoch 315 | loss is 0.421\n",
      "epoch 320 | loss is 0.096\n",
      "epoch 325 | loss is 0.107\n",
      "epoch 330 | loss is 0.141\n",
      "epoch 335 | loss is 0.065\n",
      "epoch 340 | loss is 0.052\n",
      "epoch 345 | loss is 0.094\n",
      "epoch 350 | loss is 0.148\n",
      "epoch 355 | loss is 0.058\n",
      "epoch 360 | loss is 0.065\n",
      "epoch 365 | loss is 0.239\n",
      "epoch 370 | loss is 0.096\n",
      "epoch 375 | loss is 0.042\n",
      "epoch 380 | loss is 0.129\n",
      "epoch 385 | loss is 0.062\n",
      "epoch 390 | loss is 0.081\n",
      "epoch 395 | loss is 0.073\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'SimpleNN' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     logits = model(X_test_t)\n\u001b[32m     34\u001b[39m     predictions = torch.argmax(logits, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     35\u001b[39m     accuracy = (predictions == y_test_t).float().mean()\n",
      "\u001b[31mTypeError\u001b[39m: 'SimpleNN' object is not callable"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp_model.parameters(),lr=1e-3)\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 400\n",
    "nusamples = X_train_t.shape[0]\n",
    "print(nusamples)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(nusamples)\n",
    "\n",
    "    for i in range(0,nusamples,batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_t[idx]\n",
    "\n",
    "        logits = mlp_model(X_batch)\n",
    "        loss = criterion(logits,y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'epoch {epoch} | loss is {loss.item():.3f}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "423adf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9574074149131775\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = mlp_model(X_test_t)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == y_test_t).float().mean()\n",
    "\n",
    "print(\"\\nTest accuracy:\", accuracy.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
