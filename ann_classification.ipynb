{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121cc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfaf3076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(3,4)\n",
    "\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e93bebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self,input_dim,num_neurons):\n",
    "    \n",
    "        self.w1 = torch.rand(input_dim,num_neurons,requires_grad=True)\n",
    "        self.w2 = torch.rand(num_neurons,1,requires_grad=True)\n",
    "        self.Loss = []\n",
    "        self.b1 = torch.zeros(1,num_neurons,requires_grad=True)\n",
    "        self.b2 = torch.zeros(1,1,requires_grad=True)\n",
    "\n",
    "    \n",
    "    def Tanh(self,x):\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def ReLU(self,x):\n",
    "        return torch.relu(x)\n",
    "    \n",
    "\n",
    "    def softmax(self,x):\n",
    "        ex = torch.exp(x - x.max(dim=1,keepdim=True).values)\n",
    "        return ex / ex.sum(dim=1,keepdim=True)\n",
    "\n",
    "    def cross_entropy(self,perdicted,target): \n",
    "        per_sample = - torch.sum(target * torch.log(perdicted + 1e-12),dim=1) \n",
    "        return per_sample.mean()\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,X):\n",
    "\n",
    "        self.z1 = X @ self.w1 + self.b1\n",
    "\n",
    "        self.h1 = self.ReLU(self.z1)\n",
    "\n",
    "        self.z2 = self.h1 @ self.w2 + self.b2\n",
    "\n",
    "        self.y_hat = self.softmax(self.z2)\n",
    "\n",
    "        return self.y_hat\n",
    "    \n",
    "\n",
    "\n",
    "    def back_prop(self,target,lr=1e-3):\n",
    "        loss_value =  self.cross_entropy(self.y_hat,target=target)\n",
    "        self.Loss.append(loss_value.item())\n",
    "\n",
    "        #clearing old gradient good practice\n",
    "        if self.w1.grad is not None:\n",
    "            self.w1.grad.zero_()\n",
    "            self.b1.grad.zero_()\n",
    "            self.w2.grad.zero_()\n",
    "            self.b2.grad.zero_()\n",
    "\n",
    "        #clac the gradient\n",
    "\n",
    "        loss_value.backward()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.w1 -= lr * self.w1.grad\n",
    "            self.b1 -= lr * self.b1.grad\n",
    "\n",
    "            self.w2 -= lr * self.w2.grad\n",
    "            self.b2 -= lr * self.b2.grad\n",
    "\n",
    "        #after calling torch.no_grad the gradient should be re-enabled\n",
    "        self.w1.requires_grad_(True)\n",
    "        self.b1.requires_grad_(True)\n",
    "        self.w2.requires_grad_(True)\n",
    "        self.b2.requires_grad_(True)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b0e8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, num_neurons):\n",
    "\n",
    "        self.w1 = torch.randn(input_dim, num_neurons, requires_grad=True) * 0.01\n",
    "        self.b1 = torch.zeros(1, num_neurons, requires_grad=True)\n",
    "\n",
    "        # FIXED: OUTPUT MUST BE 10 FOR DIGITS\n",
    "        self.w2 = torch.randn(num_neurons, 10, requires_grad=True) * 0.01\n",
    "        self.b2 = torch.zeros(1, 10, requires_grad=True)\n",
    "\n",
    "        self.Loss = []\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return torch.relu(x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        ex = torch.exp(x - x.max(dim=1, keepdim=True).values)\n",
    "        return ex / ex.sum(dim=1, keepdim=True)\n",
    "\n",
    "    def cross_entropy(self, predicted, target):\n",
    "        per_sample = -torch.sum(target * torch.log(predicted + 1e-12), dim=1)\n",
    "        return per_sample.mean()\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.w1 + self.b1\n",
    "        self.h1 = self.ReLU(self.z1)\n",
    "        self.z2 = self.h1 @ self.w2 + self.b2\n",
    "        self.y_hat = self.softmax(self.z2)\n",
    "        return self.y_hat\n",
    "\n",
    "    def back_prop(self, target, lr=1e-3):\n",
    "\n",
    "        loss = self.cross_entropy(self.y_hat, target)\n",
    "        self.Loss.append(float(loss.detach()))\n",
    "\n",
    "        # backwards\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        with torch.no_grad():\n",
    "            self.w1 -= lr * self.w1.grad\n",
    "            self.b1 -= lr * self.b1.grad\n",
    "            self.w2 -= lr * self.w2.grad\n",
    "            self.b2 -= lr * self.b2.grad\n",
    "\n",
    "        # clear grads\n",
    "        self.w1.grad.zero_()\n",
    "        self.b1.grad.zero_()\n",
    "        self.w2.grad.zero_()\n",
    "        self.b2.grad.zero_()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f70e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khesr\\AppData\\Local\\Temp\\ipykernel_26632\\3792310739.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(model.w1.grad, model.w2.grad)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNN(input_dim=64, num_neurons=100)\n",
    "\n",
    "print(model.w1.grad, model.w2.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khesr\\AppData\\Local\\Temp\\ipykernel_26632\\1494212508.py:42: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  self.w1 -= lr * self.w1.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     y_batch = y_train_oh[idx]\n\u001b[32m     45\u001b[39m     model.forward(X_batch)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     loss = model.back_prop(y_batch, lr=lr)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mSimpleNN.back_prop\u001b[39m\u001b[34m(self, target, lr)\u001b[39m\n\u001b[32m     39\u001b[39m loss.backward()\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28mself\u001b[39m.w1 -= lr * \u001b[38;5;28mself\u001b[39m.w1.grad\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m.b1 -= lr * \u001b[38;5;28mself\u001b[39m.b1.grad\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m.w2 -= lr * \u001b[38;5;28mself\u001b[39m.w2.grad\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm  = scaler.transform(X_test)  # FIXED\n",
    "\n",
    "# Tensors\n",
    "X_train_t = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_norm, dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# One-hot\n",
    "y_train_oh = torch.eye(10)[y_train_t]\n",
    "y_test_oh  = torch.eye(10)[y_test_t]\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_oh[idx]\n",
    "\n",
    "        model.forward(X_batch)\n",
    "        loss = model.back_prop(y_batch, lr=lr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d5d77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1797, 64)\n",
      "y shape: (1797,)\n",
      "Train: (1257, 64) | Test: (540, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset\n",
    "digits = load_digits()\n",
    "X = digits.data        # shape (1797, 64)\n",
    "y = digits.target      # labels 0â€“9\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "\n",
    "# 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05d4540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 796 1554   86  637 1048 1556  299  899 1362  273]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "idx = np.random.choice(len(X), 10, replace=False)\n",
    "print(idx)\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "for i, img_idx in enumerate(idx):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(X[img_idx].reshape(8,8), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {y[img_idx]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08b93c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.float32\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_norm = scaler.fit_transform(X_train)\n",
    "x_test_norm = scaler.fit_transform(X_test)\n",
    "\n",
    "#converting them to tensor\n",
    "\n",
    "X_train_t = torch.tensor(x_train_norm,dtype=torch.float32)\n",
    "X_test_t = torch.tensor(x_test_norm,dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train,dtype=torch.long)\n",
    "y_test_t = torch.tensor(y_test,dtype=torch.long)\n",
    "\n",
    "\n",
    "print(y_test_t.dtype,x_test_t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "967f7b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def onehot(label):\n",
    "    return torch.eye(10)[label]\n",
    "\n",
    "y_train_oh = onehot(y_train_t)\n",
    "y_test_oh  = onehot(y_test_t)\n",
    "print(y_test_oh[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ad59ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_dim=64, num_neurons=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84205805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 0.0000\n",
      "Epoch 5, Loss = 0.0000\n",
      "Epoch 10, Loss = 0.0000\n",
      "Epoch 15, Loss = 0.0000\n",
      "Epoch 20, Loss = 0.0000\n",
      "Epoch 25, Loss = 0.0000\n",
      "Epoch 30, Loss = 0.0000\n",
      "Epoch 35, Loss = 0.0000\n",
      "Epoch 40, Loss = 0.0000\n",
      "Epoch 45, Loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "num_samples = X_train_t.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # shuffle indices\n",
    "    perm = torch.randperm(num_samples)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        idx = perm[i : i + batch_size]\n",
    "\n",
    "        X_batch = X_train_t[idx]\n",
    "        y_batch = y_train_oh[idx]\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model.forward(X_batch)\n",
    "\n",
    "        # backward pass (your manual implementation)\n",
    "        model.back_prop(y_batch, lr=lr)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {model.Loss[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
