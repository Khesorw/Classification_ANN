{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1117d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Define Training Data (y = 3x + 2)\n",
    "# Features (x)\n",
    "X = torch.tensor([[1.0], [2.0], [3.0]], dtype=torch.float32)\n",
    "# Labels (y)\n",
    "Y = torch.tensor([[5.0], [8.0], [11.0]], dtype=torch.float32)\n",
    "\n",
    "# 2. Initialize Model Parameters Manually\n",
    "# We use torch.randn (random normal) or torch.rand (random uniform) for initialization\n",
    "# REQUIRES_GRAD=TRUE is the key to tracking gradients\n",
    "W = torch.randn(1, 1, requires_grad=True) # Weight (slope)\n",
    "B = torch.rand(1, requires_grad=True)     # Bias (y-intercept)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf565b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 90.0378\n",
      "Weight (W) Gradient: 0.0000\n",
      "New Weight (W): -0.4616\n",
      "New Bias (B): 0.9742\n"
     ]
    }
   ],
   "source": [
    "# --- 1. FORWARD PASS ---\n",
    "# Manual Linear Model: Y_pred = X @ W + B\n",
    "Y_pred = X @ W + B \n",
    "\n",
    "# --- 2. LOSS CALCULATION (Mean Squared Error) ---\n",
    "# We use torch.sum and torch.pow to calculate MSE manually\n",
    "loss = torch.mean(torch.pow(Y_pred - Y, 2))\n",
    "\n",
    "# --- 3. BACKWARD PASS (Gradient Computation) ---\n",
    "# Calculates d(loss)/d(W) and d(loss)/d(B) and stores them in W.grad and B.grad\n",
    "loss.backward()\n",
    "\n",
    "# --- 4. PARAMETER UPDATE (Optimization) ---\n",
    "# We update the tensors using the calculated gradients (W.grad and B.grad).\n",
    "# We wrap this in torch.no_grad() because the update itself is not part of the forward\n",
    "# calculation and should not have its gradient tracked.\n",
    "with torch.no_grad():\n",
    "    W -= learning_rate * W.grad\n",
    "    B -= learning_rate * B.grad\n",
    "    \n",
    "    # Crucial: Manually zero the gradients after the update \n",
    "    # (otherwise they accumulate from one iteration to the next)\n",
    "    W.grad.zero_()\n",
    "    B.grad.zero_()\n",
    "\n",
    "# --- 5. RESULTS ---\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Weight (W) Gradient: {W.grad.item():.4f}\")\n",
    "print(f\"New Weight (W): {W.item():.4f}\")\n",
    "print(f\"New Bias (B): {B.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Define Training Data (y = 3x + 2)\n",
    "X = torch.tensor([[1.0], [2.0], [3.0]], dtype=torch.float32)\n",
    "Y = torch.tensor([[5.0], [8.0], [11.0]], dtype=torch.float32)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "\n",
    "# --- Initialize Parameters ---\n",
    "# Create W and B with the same initial random values for a fair comparison\n",
    "W_initial = torch.randn(1, 1, requires_grad=True)\n",
    "B_initial = torch.rand(1, requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8621e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bfc816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1754]], requires_grad=True) \n",
      " tensor([0.1492], requires_grad=True)\n",
      "pytorch using step function\n",
      "initial W: 1.1754 and initial B: 0.1492\n",
      "Final Loss: 32.4693\n",
      "Final W: 1.4197, Final B: 0.2592\n",
      "\n",
      "--- Method 2: Manual Update (Replacing step()) ---\n",
      "Initial W: 1.1754, Initial B: 0.1492\n",
      "Final Loss: 32.4693\n",
      "Final W: 1.4197, Final B: 0.2592\n"
     ]
    }
   ],
   "source": [
    "# print(W_initial,'\\n',B_initial)\n",
    "W_step = W_initial.clone().detach().requires_grad_(True)\n",
    "B_step = B_initial.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(W_step,'\\n',B_step)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD([W_step,B_step],lr=learning_rate)\n",
    "\n",
    "print('pytorch using step function')\n",
    "print(f'initial W: {W_step.item():.4f} and initial B: {B_step.item():.4f}')\n",
    "\n",
    "Y_pred = X @ W_step + B_step\n",
    "\n",
    "loss_step = torch.mean(torch.pow(Y_pred - Y,2))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss_step.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "# Print results\n",
    "print(f\"Final Loss: {loss_step.item():.4f}\")\n",
    "print(f\"Final W: {W_step.item():.4f}, Final B: {B_step.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create fresh, identical copies of initial parameters\n",
    "W_manual = W_initial.clone().detach().requires_grad_(True)\n",
    "B_manual = B_initial.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Print initial state\n",
    "print(\"\\n--- Method 2: Manual Update (Replacing step()) ---\")\n",
    "print(f\"Initial W: {W_manual.item():.4f}, Initial B: {B_manual.item():.4f}\")\n",
    "\n",
    "# 1. Forward Pass\n",
    "Y_pred_manual = X @ W_manual + B_manual\n",
    "\n",
    "# 2. Loss Calculation\n",
    "loss_manual = torch.mean(torch.pow(Y_pred_manual - Y, 2))\n",
    "\n",
    "# 3. Backward Pass\n",
    "loss_manual.backward() # Computes and stores new gradients\n",
    "\n",
    "# 4. Parameter Update (Manual SGD Logic)\n",
    "with torch.no_grad():\n",
    "    # Manual update formula: parameter = parameter - lr * gradient\n",
    "    W_manual -= learning_rate * W_manual.grad\n",
    "    B_manual -= learning_rate * B_manual.grad\n",
    "    \n",
    "    # Manual zeroing of gradients\n",
    "    W_manual.grad.zero_()\n",
    "    B_manual.grad.zero_()\n",
    "\n",
    "# Print results\n",
    "print(f\"Final Loss: {loss_manual.item():.4f}\")\n",
    "print(f\"Final W: {W_manual.item():.4f}, Final B: {B_manual.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
